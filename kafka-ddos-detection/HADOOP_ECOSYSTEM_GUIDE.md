# üåê DDoS Detection System with Complete Hadoop Ecosystem Integration

## üìã Overview

This is a **production-grade, distributed DDoS detection system** that leverages the **complete Hadoop ecosystem** for big data processing, storage, and analysis. The system processes network logs in real-time, detects attacks using multiple algorithms, and stores results in a distributed manner.

---

## üéØ Hadoop Ecosystem Components Integration

### ‚úÖ All 6 Core Components Fully Integrated:

| Component | Purpose | Integration Details |
|-----------|---------|---------------------|
| **Kafka** | Stream Processing | Message broker for real-time log streaming and alert distribution |
| **HDFS** | Distributed Storage | Long-term storage of network logs with date partitioning |
| **Spark** | Distributed Computing | Real-time streaming analytics and batch ML training |
| **HBase** | NoSQL Database | Fast alert storage and retrieval with column-family design |
| **Zookeeper** | Coordination | Leader election, configuration management, service discovery |
| **Mahout-Style ML** | Machine Learning | Distributed ML training on HDFS data using Spark MLlib |

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    HADOOP ECOSYSTEM INTEGRATION                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ  Zookeeper   ‚îÇ ‚Üê Configuration & Coordination
                         ‚îÇ  (Port 2181) ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ             ‚îÇ             ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   Kafka    ‚îÇ  ‚îÇ  HBase ‚îÇ  ‚îÇ   HDFS   ‚îÇ
           ‚îÇ (Port 9092)‚îÇ  ‚îÇ (9090) ‚îÇ  ‚îÇ (9000)   ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ            ‚îÇ            ‚îÇ
                  ‚îÇ            ‚îÇ            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ         Spark Streaming & MLlib          ‚îÇ
        ‚îÇ         (Master: 7077, UI: 8081)         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ            ‚îÇ            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Rule-Based‚îÇ  ‚îÇML-Based ‚îÇ  ‚îÇ Analytics‚îÇ
        ‚îÇ Detection ‚îÇ  ‚îÇDetection‚îÇ  ‚îÇ & Reports‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Components & Scripts

### 1. **Log Producer** (`log_producer.py`)
- **Purpose**: Generate realistic network traffic
- **Kafka Integration**: Publishes logs to `network-logs` topic
- **Features**: Normal traffic, DDoS simulation, multiple IP patterns

### 2. **HDFS Storage** (`hdfs_storage.py`)
- **Purpose**: Long-term log storage
- **HDFS Integration**: Stores logs with date/hour partitioning
- **Path Structure**: `/ddos/logs/raw/year=YYYY/month=MM/day=DD/hour=HH/`
- **Features**: Batch writes, automatic partitioning

### 3. **Rule-Based Detector** (`ddos_detector.py`)
- **Purpose**: Fast threshold-based detection
- **Kafka Integration**: Consumes logs, produces alerts
- **Detection**: Request rate, error rate, response time, scanning behavior

### 4. **Spark Streaming Detector** (`spark_stream_detector.py`)
- **Purpose**: Real-time windowed analytics
- **Spark Integration**: Structured streaming with 5-minute windows
- **Kafka Integration**: Consumes logs, produces `spark-alerts`
- **Features**: Watermarking, aggregations, cluster detection

### 5. **Mahout ML Detector** (`mahout_ml_detector.py`)
- **Purpose**: Machine learning-based detection
- **Algorithms**: K-Means, Isolation Forest, Random Forest
- **Features**: Auto-training, model persistence, ensemble detection
- **Kafka Integration**: Real-time predictions

### 6. **HBase Anomaly Store** (`hbase_anomaly_store.py`)
- **Purpose**: Fast alert storage and retrieval
- **HBase Integration**: Two tables with column families
- **Tables**: 
  - `ddos_alerts`: Alert details with features
  - `ip_statistics`: Aggregated IP metrics

### 7. **Zookeeper Coordinator** (`zookeeper_coordinator.py`) ‚ú® **NEW**
- **Purpose**: Distributed coordination
- **Features**:
  - Leader election for components
  - Centralized configuration management
  - Service discovery and health monitoring
  - Distributed locks
  - Component registration

### 8. **Mahout Distributed Trainer** (`mahout_distributed_trainer.py`) ‚ú® **NEW**
- **Purpose**: Large-scale ML model training
- **Spark + HDFS Integration**: Reads from HDFS, trains with Spark MLlib
- **Models Trained**:
  - K-Means Clustering (4 clusters)
  - Bisecting K-Means (hierarchical)
  - Random Forest Classifier
  - Gradient Boosted Trees
- **Features**: Model versioning, HDFS storage, metrics evaluation

### 9. **Big Data Analyzer** (`bigdata_analyzer.py`) ‚ú® **NEW**
- **Purpose**: Comprehensive batch analytics
- **Spark + HDFS Integration**: MapReduce-style aggregations
- **Analyses**:
  - Traffic pattern analysis
  - Suspicious IP detection
  - Attack pattern identification
  - URL scanning detection
  - Response time analytics
  - Threat intelligence generation

### 10. **HBase Analytics** (`hbase_analytics.py`) ‚ú® **NEW**
- **Purpose**: Advanced threat intelligence reporting
- **HBase Integration**: Complex scans and aggregations
- **Features**:
  - Recent alerts query
  - Trend analysis
  - Top offenders identification
  - IP history tracking
  - Comprehensive threat reports
  - JSON export

### 11. **Alert Monitor** (`alert_monitor.py`)
- **Purpose**: Real-time alert visualization
- **Kafka Integration**: Consumes all alert topics

### 12. **Dashboard** (`dashboard.py`)
- **Purpose**: Web-based monitoring interface
- **Framework**: Streamlit
- **Features**: Real-time metrics, charts, packet capture view

---

## üöÄ Quick Start Guide

### Prerequisites
```powershell
# 1. Docker Desktop installed and running
# 2. Python 3.8+ installed
# 3. At least 8GB RAM available
```

### Step 1: Start Hadoop Ecosystem
```powershell
cd C:\Projects\Network_Log_Analysis\kafka-ddos-detection
docker-compose up -d
```

**Services Started:**
- Zookeeper (2181)
- Kafka (9092) + Kafka UI (8080)
- HDFS NameNode (9870) + DataNode (9864)
- Spark Master (7077, UI: 8081) + Worker (8082)
- HBase (9090, UI: 16010)

### Step 2: Create Kafka Topics
```powershell
docker exec -it kafka kafka-topics --create --topic network-logs --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
docker exec -it kafka kafka-topics --create --topic ddos-alerts --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
docker exec -it kafka kafka-topics --create --topic spark-alerts --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

### Step 3: Setup Python Environment
```powershell
# Create and activate virtual environment
python -m venv venv
venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### Step 4: Initialize Zookeeper Coordination
```powershell
# Terminal 1: Start Zookeeper coordinator (optional, for advanced features)
python scripts\zookeeper_coordinator.py --demo
```

### Step 5: Run Detection Pipeline

#### Option A: Full Pipeline (5 terminals)

**Terminal 1: HDFS Storage**
```powershell
python scripts\hdfs_storage.py
```

**Terminal 2: Rule-Based Detector**
```powershell
python scripts\ddos_detector.py
```

**Terminal 3: HBase Alert Storage**
```powershell
python scripts\hbase_anomaly_store.py
```

**Terminal 4: Log Producer**
```powershell
python scripts\log_producer.py --duration 30 --ddos-prob 0.2
```

**Terminal 5: Dashboard**
```powershell
streamlit run scripts\dashboard.py
```

#### Option B: ML-Enhanced Pipeline (7 terminals)

Add these to Option A:

**Terminal 6: Mahout ML Detector**
```powershell
python scripts\mahout_ml_detector.py
```

**Terminal 7: Spark Streaming**
```powershell
python scripts\spark_stream_detector.py --output-mode kafka
```

---

## üéì Advanced Features

### 1. Distributed ML Training (Mahout-Style)

Train ML models on HDFS data using Spark:

```powershell
# Ensure HDFS has data (run log producer first)
python scripts\log_producer.py --duration 10 --ddos-prob 0.3

# Wait for HDFS storage to write data
# Then train models
python scripts\mahout_distributed_trainer.py --sample-fraction 1.0
```

**Output:**
- 4 ML models trained on distributed data
- Models saved to HDFS `/ddos/models/`
- Evaluation metrics (accuracy, silhouette scores)

### 2. Big Data Analytics

Run comprehensive analytics on HDFS data:

```powershell
# Full analysis
python scripts\bigdata_analyzer.py --analysis all

# Specific analyses
python scripts\bigdata_analyzer.py --analysis traffic
python scripts\bigdata_analyzer.py --analysis suspicious
python scripts\bigdata_analyzer.py --analysis attacks
python scripts\bigdata_analyzer.py --analysis threat
```

**Features:**
- Traffic pattern analysis
- Suspicious IP detection
- Attack timeline reconstruction
- URL scanning detection
- Response time analytics
- Risk scoring

### 3. HBase Threat Intelligence

Generate threat intelligence reports:

```powershell
# Full threat report
python scripts\hbase_analytics.py --action report

# Top offending IPs
python scripts\hbase_analytics.py --action offenders --limit 50

# IP-specific history
python scripts\hbase_analytics.py --action ip-history --ip 45.227.253.100

# Export to JSON
python scripts\hbase_analytics.py --action export --output report.json
```

### 4. Zookeeper Coordination

Use Zookeeper for distributed coordination:

```powershell
# List active components
python scripts\zookeeper_coordinator.py --list-components

# Check component health
python scripts\zookeeper_coordinator.py --health

# Set configuration
python scripts\zookeeper_coordinator.py --set-config detection_threshold 75

# Get configuration
python scripts\zookeeper_coordinator.py --get-config detection_threshold
```

---

## üìä Data Flow

### Real-Time Processing Flow:
```
Log Producer ‚Üí Kafka (network-logs) ‚Üí Rule Detector ‚Üí Kafka (ddos-alerts)
                                   ‚Üì
                              Spark Streaming ‚Üí Kafka (spark-alerts)
                                   ‚Üì
                              ML Detector ‚Üí Kafka (ddos-alerts)
                                   ‚Üì
                              HBase Storage (fast lookups)
```

### Batch Processing Flow:
```
Log Producer ‚Üí Kafka (network-logs) ‚Üí HDFS Storage
                                          ‚Üì
                                   Spark Analytics
                                          ‚Üì
                                   ML Training (Mahout-style)
                                          ‚Üì
                                   Models ‚Üí HDFS
```

---

## üóÇÔ∏è HDFS Directory Structure

```
/ddos/
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Raw network logs (partitioned)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ year=2025/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=10/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=09/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hour=14/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logs_20251009_140523.json
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Processed analytics
‚îÇ   ‚îî‚îÄ‚îÄ alerts/                 # Historical alerts
‚îî‚îÄ‚îÄ models/                     # ML models
    ‚îú‚îÄ‚îÄ kmeans/
    ‚îú‚îÄ‚îÄ bisecting_kmeans/
    ‚îú‚îÄ‚îÄ random_forest/
    ‚îî‚îÄ‚îÄ gradient_boosting/
```

---

## üè¢ HBase Schema

### Table: `ddos_alerts`
```
Row Key: {timestamp}_{ip}

Column Families:
- alert:     timestamp, source_ip, severity, detection_method
- source:    ip_address, user_agent
- metrics:   request_rate, error_rate, response_time
- features:  url_diversity, bytes_sent, etc.
```

### Table: `ip_statistics`
```
Row Key: {ip_address}

Column Families:
- info:      ip_address, last_alert_time
- stats:     alert_count, last_severity
- history:   latest_alert (JSON)
```

---

## üß™ Testing & Validation

### 1. Test Kafka Connectivity
```powershell
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092
```

### 2. Test HDFS
```powershell
docker exec -it namenode hdfs dfs -ls /
docker exec -it namenode hdfs dfs -mkdir -p /ddos/logs/raw
```

### 3. Test HBase
```powershell
docker exec -it hbase hbase shell
> list
> scan 'ddos_alerts', {LIMIT => 5}
> exit
```

### 4. Test Spark
```powershell
docker exec -it spark-master /opt/spark/bin/spark-submit --version
```

### 5. Test Zookeeper
```powershell
docker exec -it zookeeper zkCli.sh ls /
```

---

## üìà Performance Metrics

### Throughput:
- **Kafka**: 10,000+ messages/sec
- **HDFS**: Batch writes of 100 logs
- **Spark Streaming**: 5-minute windows with 1-minute slides
- **HBase**: Sub-second alert lookups

### Scalability:
- **Horizontal**: Add more Spark workers, Kafka partitions
- **Vertical**: Increase executor memory, worker cores

---

## üîß Configuration Files

### `config/pipeline_config.json`
- Kafka brokers and topics
- HDFS paths
- HBase tables and column families
- Spark configurations
- Detection thresholds
- Component enable/disable flags

### `docker-compose.yml`
- All Hadoop ecosystem services
- Port mappings
- Volume mounts
- Network configuration

---

## üìö Component Details

### Kafka (Stream Processing)
- **Topics**: `network-logs`, `ddos-alerts`, `spark-alerts`
- **Partitions**: 3 per topic
- **Replication**: 1 (single broker)
- **Compression**: Snappy

### HDFS (Distributed Storage)
- **Replication Factor**: 1
- **Block Size**: 128MB (default)
- **NameNode**: Port 9870 (Web UI), 9000 (RPC)
- **DataNode**: Port 9864

### Spark (Distributed Computing)
- **Master**: spark://localhost:7077
- **Worker Memory**: 2GB
- **Worker Cores**: 2
- **Executor Memory**: 2GB
- **Shuffle Partitions**: 8

### HBase (NoSQL Database)
- **Thrift Server**: Port 9090
- **Master UI**: Port 16010
- **RegionServer**: Port 16020
- **Connection Pool**: happybase

### Zookeeper (Coordination)
- **Client Port**: 2181
- **Used By**: Kafka, HBase, Custom coordination
- **Data Directory**: `/zookeeper/data`

---

## üéØ Use Cases

1. **Real-Time DDoS Detection**: Sub-second alert generation
2. **Historical Analysis**: Query weeks of data in seconds
3. **Threat Intelligence**: Identify patterns and repeat offenders
4. **Capacity Planning**: Analyze traffic trends
5. **Compliance Reporting**: Export detailed audit logs
6. **ML Model Training**: Continuous improvement with new data

---

## üõ†Ô∏è Troubleshooting

### Kafka Issues
```powershell
# Check broker
docker logs kafka

# Verify topics
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092
```

### HDFS Issues
```powershell
# Check NameNode
docker logs namenode

# Safe mode status
docker exec -it namenode hdfs dfsadmin -safemode get
```

### HBase Issues
```powershell
# Check HBase
docker logs hbase

# Verify Thrift server
curl http://localhost:9090
```

### Spark Issues
```powershell
# Check master
docker logs spark-master

# Check worker
docker logs spark-worker

# View Spark UI
http://localhost:8081
```

---

## üìñ References

- **Apache Kafka**: https://kafka.apache.org/
- **Apache Hadoop (HDFS)**: https://hadoop.apache.org/
- **Apache Spark**: https://spark.apache.org/
- **Apache HBase**: https://hbase.apache.org/
- **Apache Zookeeper**: https://zookeeper.apache.org/
- **Apache Mahout**: https://mahout.apache.org/

---

## üéì Learning Resources

### Beginner:
1. Run the full pipeline with dashboard
2. Observe real-time detection
3. Query HBase for alerts
4. View HDFS data structure

### Intermediate:
1. Train ML models with Mahout trainer
2. Run big data analytics
3. Generate threat intelligence reports
4. Modify detection thresholds

### Advanced:
1. Implement custom ML algorithms
2. Add new Spark streaming jobs
3. Create custom HBase scans
4. Extend Zookeeper coordination

---

## üìù License

This project is for educational purposes demonstrating Hadoop ecosystem integration.

---

## üë• Contributors

Built as a comprehensive demonstration of Hadoop ecosystem components for big data analytics and real-time stream processing.

---

**Last Updated**: October 9, 2025  
**Version**: 2.0 (Complete Hadoop Ecosystem Integration)  
**Status**: ‚úÖ Production-Ready with all 6 core components integrated
